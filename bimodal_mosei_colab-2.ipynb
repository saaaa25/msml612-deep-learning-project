{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f60f6fc",
      "metadata": {
        "id": "0f60f6fc"
      },
      "source": [
        "# Bimodal MOSEI (Text + Audio) Trainer ‚Äî **No MMSDK**, Colab-Ready\n",
        "\n",
        "This notebook:\n",
        "- Reads `.csd` (HDF5) files **directly via `h5py`** (no `mmsdk`).\n",
        "- Supports **acoustic file in chunks** and merges them before use.\n",
        "- Trains a small PyTorch regressor on **text + audio** features (mean-pooled per segment).\n",
        "- Uses a simple 80/10/10 split.\n",
        "\n",
        "**Setup tips**\n",
        "- Put your dataset under `/content/dataset` or mount Google Drive.\n",
        "- Expected paths (customize later):\n",
        "  - `dataset/languages/CMU_MOSEI_TimestampedWordVectors.csd`\n",
        "  - `dataset/acoustics/CMU_MOSEI_COVAREP.csd` (or chunks in that folder)\n",
        "  - `dataset/labels/CMU_MOSEI_Labels.csd`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Environment check & installs\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"PyTorch CUDA version:\", torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkU_ejHQK78X",
        "outputId": "76628de0-8df8-4fa6-df8d-c84f3cdb8b85"
      },
      "id": "PkU_ejHQK78X",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  6 08:03:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA available: True\n",
            "PyTorch CUDA version: 12.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "437fed14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "437fed14",
        "outputId": "bf4f22f6-6603-46c7-b3d3-7256c0d322c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted. Example path: /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset\n"
          ]
        }
      ],
      "source": [
        "#@title 2) (Optional) Mount Google Drive\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print('Drive mounted. Example path: /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset')\n",
        "except Exception as e:\n",
        "    print('Drive mount skipped or failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a5357865",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5357865",
        "outputId": "a2afa568-3bcc-430f-c22a-062ba1a7145a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT = /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset\n",
            "LANGUAGE  = /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/languages/CMU_MOSEI_TimestampedWordVectors.csd\n",
            "ACOUSTIC  = /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/acoustics/CMU_MOSEI_COVAREP.csd\n",
            "LABELS    = /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/labels/CMU_MOSEI_Labels.csd\n"
          ]
        }
      ],
      "source": [
        "#@title 3) Paths ‚Äî set your dataset location here\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = Path('/content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset')  # e.g., Path('/content/drive/MyDrive/mosei/dataset')\n",
        "\n",
        "LANGUAGE = DATA_ROOT / 'languages/CMU_MOSEI_TimestampedWordVectors.csd'\n",
        "ACOUSTIC = DATA_ROOT / 'acoustics/CMU_MOSEI_COVAREP.csd'\n",
        "LABELS   = DATA_ROOT / 'labels/CMU_MOSEI_Labels.csd'\n",
        "\n",
        "ACOUSTIC_CHUNKS_DIR = ACOUSTIC.parent\n",
        "\n",
        "print('DATA_ROOT =', DATA_ROOT)\n",
        "print('LANGUAGE  =', LANGUAGE)\n",
        "print('ACOUSTIC  =', ACOUSTIC)\n",
        "print('LABELS    =', LABELS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1fb04fe9",
      "metadata": {
        "id": "1fb04fe9"
      },
      "outputs": [],
      "source": [
        "#@title 4) Utils: chunk merge, HDF5 readers, dataset\n",
        "import os, time, random, shutil\n",
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "os.environ.setdefault('HDF5_USE_FILE_LOCKING', 'FALSE')\n",
        "\n",
        "def log(msg: str):\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
        "\n",
        "def detect_and_merge_chunks(chunks_dir: Path, base_filename: str, output_dir: Path) -> Path:\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_path = output_dir / base_filename\n",
        "    patterns = ['*.csdchunk', 'part_*', '*.chunk']\n",
        "    chunk_files = []\n",
        "    for pat in patterns:\n",
        "        matches = sorted(chunks_dir.glob(pat))\n",
        "        matches = [m for m in matches if m.suffix != '.csd' and m.is_file()]\n",
        "        if matches:\n",
        "            chunk_files = matches\n",
        "            break\n",
        "    if not chunk_files:\n",
        "        if out_path.exists():\n",
        "            log(f'No chunks found; using existing {out_path}')\n",
        "            return out_path\n",
        "        log('No chunk files found. Skipping merge.')\n",
        "        return out_path\n",
        "    total_size = sum(cf.stat().st_size for cf in chunk_files)\n",
        "    if out_path.exists() and out_path.stat().st_size == total_size:\n",
        "        log(f'Merged file already present with matching size: {out_path}')\n",
        "        return out_path\n",
        "    log(f'Merging {len(chunk_files)} chunks from {chunks_dir} -> {out_path}')\n",
        "    with open(out_path, 'wb') as w:\n",
        "        for idx, cf in enumerate(chunk_files):\n",
        "            with open(cf, 'rb') as r:\n",
        "                shutil.copyfileobj(r, w, length=1024*1024)\n",
        "            if (idx + 1) % 10 == 0 or idx == len(chunk_files)-1:\n",
        "                log(f'  merged {idx+1}/{len(chunk_files)}')\n",
        "    log('Merge complete.')\n",
        "    return out_path\n",
        "\n",
        "def choose_overlap_keys(lang_path: Path, acou_path: Path, labl_path: Path):\n",
        "    with h5py.File(lang_path, 'r') as fl, h5py.File(acou_path, 'r') as fa, h5py.File(labl_path, 'r') as fb:\n",
        "        def segs(f):\n",
        "            out = []\n",
        "            def visit(name, obj):\n",
        "                if isinstance(obj, h5py.Group) and 'features' in obj:\n",
        "                    out.append(name)\n",
        "            f.visititems(visit)\n",
        "            return set(out)\n",
        "        L = segs(fl); A = segs(fa); B = segs(fb)\n",
        "    keys = sorted(list(L & A & B))\n",
        "    return keys\n",
        "\n",
        "def read_feature_mean(h5path: Path, seg: str) -> np.ndarray:\n",
        "    with h5py.File(h5path, 'r') as f:\n",
        "        ds = f[seg]['features']\n",
        "        arr = ds[()]\n",
        "    arr = np.asarray(arr)\n",
        "    return arr.astype(np.float32) if arr.ndim == 1 else arr.astype(np.float32).mean(axis=0)\n",
        "\n",
        "class MoseiBimodalH5(Dataset):\n",
        "    def __init__(self, lang_csd: Path, acou_csd: Path, labl_csd: Path, split: str, seed: int = 1337):\n",
        "        self.lang = Path(lang_csd); self.acou = Path(acou_csd); self.labl = Path(labl_csd)\n",
        "        all_keys = choose_overlap_keys(self.lang, self.acou, self.labl)\n",
        "        if not all_keys:\n",
        "            raise RuntimeError('No overlapping segments with features across the three CSD files.')\n",
        "        random.Random(seed).shuffle(all_keys)\n",
        "        n = len(all_keys); n_tr = int(0.8*n); n_va = int(0.1*n)\n",
        "        if split == 'train':\n",
        "            self.keys = all_keys[:n_tr]\n",
        "        elif split == 'valid':\n",
        "            self.keys = all_keys[n_tr:n_tr+n_va]\n",
        "        else:\n",
        "            self.keys = all_keys[n_tr+n_va:]\n",
        "        xL = read_feature_mean(self.lang, self.keys[0])\n",
        "        xA = read_feature_mean(self.acou, self.keys[0])\n",
        "        self.lang_dim = int(xL.shape[-1]); self.acou_dim = int(xA.shape[-1])\n",
        "    def __len__(self): return len(self.keys)\n",
        "    def __getitem__(self, idx):\n",
        "        k = self.keys[idx]\n",
        "        xL = read_feature_mean(self.lang, k)\n",
        "        xA = read_feature_mean(self.acou, k)\n",
        "        y  = read_feature_mean(self.labl, k).mean().astype(np.float32)\n",
        "        x  = np.concatenate([xL, xA], axis=-1).astype(np.float32)\n",
        "        return torch.from_numpy(x), torch.tensor([y], dtype=torch.float32)\n",
        "\n",
        "class SimpleRegressor(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_epoch(model, loader, device, optim, loss_fn):\n",
        "    model.train(); total=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        optim.zero_grad(); pred = model(xb); loss = loss_fn(pred, yb)\n",
        "        loss.backward(); optim.step(); total += loss.item()*xb.size(0)\n",
        "    return total/len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, device, loss_fn):\n",
        "    model.eval(); total=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        pred = model(xb); loss = loss_fn(pred, yb)\n",
        "        total += loss.item()*xb.size(0)\n",
        "    return total/len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "11301b60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11301b60",
        "outputId": "d828b9d9-18c4-4b1d-b92e-10a5ccb7b4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acoustic .csd already exists: /content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/acoustics/CMU_MOSEI_COVAREP.csd\n",
            "/content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/languages/CMU_MOSEI_TimestampedWordVectors.csd exists = True\n",
            "/content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/acoustics/CMU_MOSEI_COVAREP.csd exists = True\n",
            "/content/drive/MyDrive/Colab Notebooks/cmu_mosei_dataset/labels/CMU_MOSEI_Labels.csd exists = True\n"
          ]
        }
      ],
      "source": [
        "#@title 5) Merge acoustic chunks (if any) and check files\n",
        "if not ACOUSTIC.exists():\n",
        "    print('Acoustic .csd not found. Attempting to merge chunks...')\n",
        "    merged = detect_and_merge_chunks(ACOUSTIC_CHUNKS_DIR, ACOUSTIC.name, ACOUSTIC.parent)\n",
        "    print('Merged path:', merged)\n",
        "else:\n",
        "    print('Acoustic .csd already exists:', ACOUSTIC)\n",
        "\n",
        "for p in [LANGUAGE, ACOUSTIC, LABELS]:\n",
        "    print(p, 'exists =', p.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import h5py, itertools\n",
        "\n",
        "def feature_groups(h5path: Path):\n",
        "    out_full = []\n",
        "    out_leaf = []\n",
        "    with h5py.File(h5path, \"r\") as f:\n",
        "        def visit(name, obj):\n",
        "            if isinstance(obj, h5py.Group) and \"features\" in obj:\n",
        "                out_full.append(name)\n",
        "                out_leaf.append(name.split(\"/\")[-1])\n",
        "        f.visititems(visit)\n",
        "    return set(out_full), set(out_leaf)\n",
        "\n",
        "lang_full, lang_leaf = feature_groups(LANGUAGE)\n",
        "acou_full, acou_leaf = feature_groups(ACOUSTIC)\n",
        "labl_full, labl_leaf = feature_groups(LABELS)\n",
        "\n",
        "print(\"Counts (full-path):\", len(lang_full), len(acou_full), len(labl_full))\n",
        "print(\"Counts (leaf-name):\", len(lang_leaf), len(acou_leaf), len(labl_leaf))\n",
        "\n",
        "print(\"\\nLeaf-name intersections:\")\n",
        "leaf_inter = lang_leaf & acou_leaf & labl_leaf\n",
        "print(\"  intersection size:\", len(leaf_inter))\n",
        "print(\"  examples:\", list(itertools.islice(sorted(leaf_inter), 10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW5WJlegNh8-",
        "outputId": "8e13a3a7-5d0c-4a87-b2d8-3f01dac06400"
      },
      "id": "IW5WJlegNh8-",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts (full-path): 3837 3836 3293\n",
            "Counts (leaf-name): 3837 3836 3293\n",
            "\n",
            "Leaf-name intersections:\n",
            "  intersection size: 3292\n",
            "  examples: ['--qXJuDtHPw', '-3g5yACwYnA', '-3nNcZdcdvU', '-571d8cVauQ', '-6rXp3zJ3kc', '-9YyBTjo1zo', '-9y-fZ3swSY', '-AUZQgSxyPQ', '-Alixo7euuU', '-Eqdz5y4pEY']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, random, shutil\n",
        "import h5py, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "def _collect_leaf_to_full(h5path):\n",
        "    \"\"\"Map leaf segment name -> full HDF5 path containing 'features'.\"\"\"\n",
        "    map_ = {}\n",
        "    with h5py.File(h5path, \"r\") as f:\n",
        "        def visit(name, obj):\n",
        "            if isinstance(obj, h5py.Group) and \"features\" in obj:\n",
        "                leaf = name.split(\"/\")[-1]\n",
        "                # keep first occurrence; assuming leaf names are unique enough across files\n",
        "                map_.setdefault(leaf, name)\n",
        "        f.visititems(visit)\n",
        "    return map_\n",
        "\n",
        "def _read_feature_mean(h5path: Path, fullpath: str):\n",
        "    with h5py.File(h5path, \"r\") as f:\n",
        "        arr = f[fullpath][\"features\"][()]\n",
        "    arr = np.asarray(arr)\n",
        "    return arr.astype(np.float32) if arr.ndim == 1 else arr.astype(np.float32).mean(axis=0)\n",
        "\n",
        "class MoseiBimodalH5(Dataset):\n",
        "    \"\"\"\n",
        "    Aligns by LEAF group name (the final path component that holds 'features'),\n",
        "    which is typically consistent across LANGUAGE / ACOUSTIC / LABELS CSDs.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang_csd, acou_csd, labl_csd, split=\"train\", seed=1337):\n",
        "        self.lang_p = Path(lang_csd); self.acou_p = Path(acou_csd); self.labl_p = Path(labl_csd)\n",
        "\n",
        "        lang_map = _collect_leaf_to_full(self.lang_p)\n",
        "        acou_map = _collect_leaf_to_full(self.acou_p)\n",
        "        labl_map = _collect_leaf_to_full(self.labl_p)\n",
        "\n",
        "        common_leaf = set(lang_map) & set(acou_map) & set(labl_map)\n",
        "        if not common_leaf:\n",
        "            raise RuntimeError(\"Still no overlap after leaf-name alignment. Check that the three CSDs correspond to the same MOSEI split/version.\")\n",
        "\n",
        "        keys = sorted(common_leaf)\n",
        "        random.Random(seed).shuffle(keys)\n",
        "        n = len(keys); n_tr = int(0.8*n); n_va = int(0.1*n)\n",
        "        if split == \"train\":\n",
        "            self.keys = keys[:n_tr]\n",
        "        elif split == \"valid\":\n",
        "            self.keys = keys[n_tr:n_tr+n_va]\n",
        "        else:\n",
        "            self.keys = keys[n_tr+n_va:]\n",
        "\n",
        "        # store the fullpath per leaf for fast access\n",
        "        self.lang_map, self.acou_map, self.labl_map = lang_map, acou_map, labl_map\n",
        "\n",
        "        # infer dims\n",
        "        k0 = self.keys[0]\n",
        "        xL = _read_feature_mean(self.lang_p, self.lang_map[k0])\n",
        "        xA = _read_feature_mean(self.acou_p, self.acou_map[k0])\n",
        "        self.lang_dim = int(xL.shape[-1]); self.acou_dim = int(xA.shape[-1])\n",
        "\n",
        "    def __len__(self): return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        leaf = self.keys[idx]\n",
        "        xL = _read_feature_mean(self.lang_p, self.lang_map[leaf])\n",
        "        xA = _read_feature_mean(self.acou_p, self.acou_map[leaf])\n",
        "        y  = _read_feature_mean(self.labl_p, self.labl_map[leaf]).mean().astype(np.float32)\n",
        "        x  = np.concatenate([xL, xA], axis=-1).astype(np.float32)\n",
        "        return torch.from_numpy(x), torch.tensor([y], dtype=torch.float32)\n",
        "\n",
        "class SimpleRegressor(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "UrkqjD9TNjMw"
      },
      "id": "UrkqjD9TNjMw",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === NaN/empty-safe dataset + training guards ===\n",
        "import h5py, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import time, random\n",
        "\n",
        "def log(msg): print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
        "\n",
        "def _collect_leaf_to_full(h5path: Path):\n",
        "    m = {}\n",
        "    with h5py.File(h5path, \"r\") as f:\n",
        "        def visit(name, obj):\n",
        "            if isinstance(obj, h5py.Group) and \"features\" in obj:\n",
        "                leaf = name.split(\"/\")[-1]\n",
        "                m.setdefault(leaf, name)\n",
        "        f.visititems(visit)\n",
        "    return m\n",
        "\n",
        "def _read_feat(h5path: Path, fullpath: str):\n",
        "    with h5py.File(h5path, \"r\") as f:\n",
        "        arr = f[fullpath][\"features\"][()]\n",
        "    arr = np.asarray(arr)\n",
        "    # Safe pooling: handle empty & NaNs\n",
        "    if arr.size == 0:\n",
        "        return None\n",
        "    if arr.ndim == 1:\n",
        "        x = arr.astype(np.float32)\n",
        "    else:\n",
        "        x = np.nanmean(arr.astype(np.float32), axis=0)\n",
        "    if not np.all(np.isfinite(x)):\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return x\n",
        "\n",
        "def _read_label_scalar(h5path: Path, fullpath: str):\n",
        "    with h5py.File(h5path, \"r\") as f:\n",
        "        arr = f[fullpath][\"features\"][()]\n",
        "    arr = np.asarray(arr, dtype=np.float32)\n",
        "    if arr.size == 0:\n",
        "        return None\n",
        "    y = float(np.nanmean(arr))\n",
        "    if not np.isfinite(y):\n",
        "        return None\n",
        "    return y\n",
        "\n",
        "class MoseiBimodalH5Safe(Dataset):\n",
        "    def __init__(self, lang_csd, acou_csd, labl_csd, split=\"train\", seed=1337, max_scan=None):\n",
        "        self.lang_p, self.acou_p, self.labl_p = Path(lang_csd), Path(acou_csd), Path(labl_csd)\n",
        "        Lmap, Amap, Bmap = _collect_leaf_to_full(self.lang_p), _collect_leaf_to_full(self.acou_p), _collect_leaf_to_full(self.labl_p)\n",
        "        common = sorted(set(Lmap) & set(Amap) & set(Bmap))\n",
        "        if not common:\n",
        "            raise RuntimeError(\"No common leaf keys across CSDs. Did you run the leaf-name version earlier?\")\n",
        "\n",
        "        # Scan and keep only segments with finite features & labels\n",
        "        rng = random.Random(seed)\n",
        "        rng.shuffle(common)\n",
        "        if max_scan:  # speed limiter for massive scans (optional)\n",
        "            common = common[:max_scan]\n",
        "\n",
        "        valid = []\n",
        "        lang_dim = acou_dim = None\n",
        "        kept = 0\n",
        "        for leaf in common:\n",
        "            xL = _read_feat(self.lang_p, Lmap[leaf])\n",
        "            xA = _read_feat(self.acou_p, Amap[leaf])\n",
        "            y  = _read_label_scalar(self.labl_p, Bmap[leaf])\n",
        "            if xL is None or xA is None or y is None:\n",
        "                continue\n",
        "            if lang_dim is None: lang_dim = int(xL.shape[-1])\n",
        "            if acou_dim is None: acou_dim = int(xA.shape[-1])\n",
        "            # Final safety: enforce dims\n",
        "            if xL.shape[-1] != lang_dim or xA.shape[-1] != acou_dim:\n",
        "                continue\n",
        "            valid.append(leaf)\n",
        "            kept += 1\n",
        "\n",
        "        if kept == 0:\n",
        "            raise RuntimeError(\"All samples filtered out as non-finite/empty.\")\n",
        "\n",
        "        n = len(valid); n_tr = int(0.8*n); n_va = int(0.1*n)\n",
        "        if split == \"train\":\n",
        "            self.keys = valid[:n_tr]\n",
        "        elif split == \"valid\":\n",
        "            self.keys = valid[n_tr:n_tr+n_va]\n",
        "        else:\n",
        "            self.keys = valid[n_tr+n_va:]\n",
        "\n",
        "        self.Lmap, self.Amap, self.Bmap = Lmap, Amap, Bmap\n",
        "        # Save dims\n",
        "        self.lang_dim, self.acou_dim = lang_dim, acou_dim\n",
        "        log(f\"Dataset({split}) ‚Äî kept {len(self.keys)} samples | lang_dim={self.lang_dim}, acou_dim={self.acou_dim}\")\n",
        "\n",
        "    def __len__(self): return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        leaf = self.keys[idx]\n",
        "        xL = _read_feat(self.lang_p, self.Lmap[leaf])\n",
        "        xA = _read_feat(self.acou_p, self.Amap[leaf])\n",
        "        y  = _read_label_scalar(self.labl_p, self.Bmap[leaf])\n",
        "        # After filtering these should be valid; still guard:\n",
        "        if xL is None or xA is None or y is None:\n",
        "            # Return zeroed fallback to avoid crashing batch; label 0\n",
        "            # (rare; mainly if file changed after scan)\n",
        "            xL = np.zeros(self.lang_dim, np.float32)\n",
        "            xA = np.zeros(self.acou_dim, np.float32)\n",
        "            y  = 0.0\n",
        "        x = np.concatenate([xL, xA], axis=-1).astype(np.float32)\n",
        "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return torch.from_numpy(x), torch.tensor([y], dtype=torch.float32)\n",
        "\n",
        "# Replace your dataset references:\n",
        "# train_ds = MoseiBimodalH5Safe(LANGUAGE, ACOUSTIC, LABELS, split='train')\n",
        "# valid_ds = MoseiBimodalH5Safe(LANGUAGE, ACOUSTIC, LABELS, split='valid')\n",
        "\n",
        "# Optional training-loop guards (paste into your train cell if you like):\n",
        "def safe_train_epoch(model, loader, device, optim, loss_fn, clip=1.0):\n",
        "    model.train(); tot=0.0; nobs=0\n",
        "    for xb, yb in loader:\n",
        "        xb = torch.nan_to_num(xb, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
        "        yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
        "        mask = torch.isfinite(xb).all(dim=1) & torch.isfinite(yb).squeeze(1)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "        xb, yb = xb[mask], yb[mask]\n",
        "        optim.zero_grad()\n",
        "        pred = model(xb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "        if not torch.isfinite(loss):  # guard explode\n",
        "            continue\n",
        "        loss.backward()\n",
        "        if clip: nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optim.step()\n",
        "        bs = xb.size(0); tot += loss.item()*bs; nobs += bs\n",
        "    return (tot/nobs) if nobs else float('nan')\n",
        "\n",
        "@torch.no_grad()\n",
        "def safe_eval_epoch(model, loader, device, loss_fn):\n",
        "    model.eval(); tot=0.0; nobs=0\n",
        "    for xb, yb in loader:\n",
        "        xb = torch.nan_to_num(xb, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
        "        yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
        "        mask = torch.isfinite(xb).all(dim=1) & torch.isfinite(yb).squeeze(1)\n",
        "        if not mask.any():\n",
        "            continue\n",
        "        xb, yb = xb[mask], yb[mask]\n",
        "        pred = model(xb)\n",
        "        loss = loss_fn(pred, yb)\n",
        "        if not torch.isfinite(loss):\n",
        "            continue\n",
        "        bs = xb.size(0); tot += loss.item()*bs; nobs += bs\n",
        "    return (tot/nobs) if nobs else float('nan')\n",
        "\n",
        "log(\"Patched dataset & guards loaded. Now re-run the Train cell using MoseiBimodalH5Safe and safe_*_epoch.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyNmsfavNiJI",
        "outputId": "d6fb27f5-0cf8-40a5-ec03-954f95e4394a"
      },
      "id": "JyNmsfavNiJI",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-11-06 08:24:03] Patched dataset & guards loaded. Now re-run the Train cell using MoseiBimodalH5Safe and safe_*_epoch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "10fde5ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10fde5ca",
        "outputId": "78e004fe-f520-4805-8815-faceb2d56b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "[2025-11-06 08:30:06] Dataset(train) ‚Äî kept 2633 samples | lang_dim=300, acou_dim=74\n",
            "[2025-11-06 08:34:14] Dataset(valid) ‚Äî kept 329 samples | lang_dim=300, acou_dim=74\n",
            "[2025-11-06 08:37:04] Epoch 1: train_loss=0.1612  valid_loss=0.0432\n",
            "[2025-11-06 08:37:04]   saved best.pt\n",
            "[2025-11-06 08:39:50] Epoch 2: train_loss=0.0217  valid_loss=0.0213\n",
            "[2025-11-06 08:39:50]   saved best.pt\n",
            "[2025-11-06 08:42:38] Epoch 3: train_loss=0.0178  valid_loss=0.0218\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "#@title 6) Train (configure hyperparameters here)\n",
        "EPOCHS = 3 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "LR = 1e-3 #@param {type:\"number\"}\n",
        "NUM_WORKERS = 2 #@param {type:\"integer\"}\n",
        "OUTDIR = 'runs_bimodal_h5' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "train_ds = MoseiBimodalH5Safe(LANGUAGE, ACOUSTIC, LABELS, split='train')\n",
        "valid_ds = MoseiBimodalH5Safe(LANGUAGE, ACOUSTIC, LABELS, split='valid')\n",
        "\n",
        "train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "valid_ld = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "in_dim = train_ds.lang_dim + train_ds.acou_dim\n",
        "model = SimpleRegressor(in_dim).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "best = float('inf')\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = safe_train_epoch(model, train_ld, device, optim, loss_fn, clip=1.0)\n",
        "    va = safe_eval_epoch(model, valid_ld, device, loss_fn)\n",
        "    log(f'Epoch {ep}: train_loss={tr:.4f}  valid_loss={va:.4f}')\n",
        "    if va < best:\n",
        "        best = va\n",
        "        torch.save({'model': model.state_dict(), 'in_dim': in_dim}, f'{OUTDIR}/best.pt')\n",
        "        log('  saved best.pt')\n",
        "print('Done.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a8454e57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8454e57",
        "outputId": "158737af-01d6-426d-a1da-b8e5e8604eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: pred[0:3]=[0.14582844 0.17016882 0.14452246]  y[0:3]=[0.52380955 0.15238096 0.09047619]\n",
            "Batch 1: pred[0:3]=[0.11185965 0.23540294 0.10852221]  y[0:3]=[0.14285715 0.2190476  0.3904762 ]\n",
            "Batch 2: pred[0:3]=[0.17836097 0.09962033 0.12599298]  y[0:3]=[0.12698413 0.07142857 0.21428572]\n"
          ]
        }
      ],
      "source": [
        "#@title 7) (Optional) Quick inference on a few samples\n",
        "model.eval()\n",
        "from itertools import islice\n",
        "for i, (xb, yb) in enumerate(islice(valid_ld, 3)):\n",
        "    with torch.no_grad():\n",
        "        pred = model(xb.to(device)).cpu().numpy().ravel()\n",
        "    print(f'Batch {i}: pred[0:3]={pred[:3]}  y[0:3]={yb.numpy().ravel()[:3]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8) Testing"
      ],
      "metadata": {
        "id": "f3XZCqSTWAE2"
      },
      "id": "f3XZCqSTWAE2",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-cell test evaluation (MSE + Pearson) ‚Äî run after training\n",
        "import torch, numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "try:\n",
        "    from scipy.stats import pearsonr\n",
        "except Exception:\n",
        "    %pip -q install scipy\n",
        "    from scipy.stats import pearsonr\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Load best checkpoint into your existing `model`\n",
        "ckpt = torch.load('runs_bimodal_h5/best.pt', map_location=device)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "model.to(device).eval()\n",
        "\n",
        "# Build test dataset/loader (uses the NaN-safe dataset you added)\n",
        "test_ds = MoseiBimodalH5Safe(LANGUAGE, ACOUSTIC, LABELS, split='test')\n",
        "test_ld = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Eval loop with NaN/Inf guards\n",
        "def _eval_loop(m, ld):\n",
        "    m.eval(); tot=0.0; n=0; preds=[]; trues=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in ld:\n",
        "            xb = torch.nan_to_num(xb, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
        "            yb = torch.nan_to_num(yb, nan=0.0, posinf=0.0, neginf=0.0).to(device)\n",
        "            mask = torch.isfinite(xb).all(dim=1) & torch.isfinite(yb).squeeze(1)\n",
        "            if not mask.any():\n",
        "                continue\n",
        "            xb, yb = xb[mask], yb[mask]\n",
        "            out = m(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            bs = xb.size(0)\n",
        "            tot += loss.item() * bs; n += bs\n",
        "            preds.append(out.detach().cpu().numpy().ravel())\n",
        "            trues.append(yb.detach().cpu().numpy().ravel())\n",
        "    preds = np.concatenate(preds) if preds else np.array([])\n",
        "    trues = np.concatenate(trues) if trues else np.array([])\n",
        "    return (tot/n if n else float('nan')), preds, trues\n",
        "\n",
        "mse, preds, trues = _eval_loop(model, test_ld)\n",
        "mask = np.isfinite(preds) & np.isfinite(trues)\n",
        "r = pearsonr(preds[mask], trues[mask])[0] if mask.any() else float('nan')\n",
        "\n",
        "print(f\"Test MSE: {mse:.4f} | Pearson r: {r:.3f} | Samples: {mask.sum()}/{len(test_ds)}\")\n",
        "print(\"Sample preds vs gold:\", list(zip(preds[:5].round(3), trues[:5].round(3))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lffuCr13WHzp",
        "outputId": "e948dce3-a68b-4c12-ae67-5011ae703fdd"
      },
      "id": "lffuCr13WHzp",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-11-06 08:50:38] Dataset(test) ‚Äî kept 330 samples | lang_dim=300, acou_dim=74\n",
            "Test MSE: 0.0205 | Pearson r: 0.370 | Samples: 330/330\n",
            "Sample preds vs gold: [(np.float32(0.141), np.float32(0.286)), (np.float32(0.263), np.float32(0.231)), (np.float32(0.146), np.float32(0.238)), (np.float32(0.17), np.float32(0.017)), (np.float32(0.232), np.float32(0.196))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio librosa transformers soundfile torch\n",
        "import gradio as gr, torch, librosa, numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load text embedding model\n",
        "tok = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "txt_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval().to(device)\n",
        "\n",
        "def voice_to_emotion(audio, text):\n",
        "    # 1Ô∏è‚É£ Extract audio features (MFCC mean)\n",
        "    if audio is None:\n",
        "        return \"Please record or upload an audio clip.\"\n",
        "    y, sr = librosa.load(audio, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=74)\n",
        "    audio_feat = np.mean(mfcc, axis=1)\n",
        "\n",
        "    # 2Ô∏è‚É£ Get text embedding (optional)\n",
        "    if text.strip():\n",
        "        inputs = tok(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            text_emb = txt_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
        "    else:\n",
        "        text_emb = np.zeros(300)  # same dim as language feature space\n",
        "\n",
        "    # 3Ô∏è‚É£ Concatenate & predict\n",
        "    x = np.concatenate([text_emb, audio_feat]).astype(np.float32)\n",
        "    x = torch.tensor(x).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        pred = model(x).cpu().item()\n",
        "\n",
        "    # 4Ô∏è‚É£ Interpret score\n",
        "    emo = \"üòä Positive\" if pred > 0.6 else \"üòê Neutral\" if pred > 0.4 else \"üòû Negative\"\n",
        "    return f\"{emo}  (score = {pred:.2f})\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=voice_to_emotion,\n",
        "    inputs=[\n",
        "        gr.Audio(label=\"üéôÔ∏è Speak or Upload Audio\", type=\"filepath\"),\n",
        "        gr.Textbox(label=\"‚úçÔ∏è Transcript (optional)\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Bimodal Voice + Text Emotion Demo\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "j4mZLaF1X-wd",
        "outputId": "4c638cc2-3a6d-4708-e22b-a63fe231d793"
      },
      "id": "j4mZLaF1X-wd",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://97b4199f180a99de50.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://97b4199f180a99de50.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}